{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Project Predictive Modeling: MVP\n",
    "### Predicting future stock price returns. *(a Kaggle competition from sigma2)*\n",
    "\n",
    "## Introduction\n",
    "\n",
    "***Can we actually predict stock prices with Machine Learning?***\n",
    "\n",
    "Investors make educated guesses by analyzing data. They'll read the news, study the company history, industry, trends...  There are lots of data points that go into malking a predction. \n",
    "The prevailing theory is that stock prices are totally random and unpredicatble. A blind-folded monkey throwing darts at a newspaper's financial pages could select a protfolio that could do just as well as one carefully selected by experts. \n",
    "\n",
    "But that raises the question, why do top firms like Morgan Stanley and citigroup hire quantitative analysts to go build predictive models. We have this idea of a trading floor filled with adrenalined-infused men, with loose ties, running around, yelling something into a phone. But these days are more likely to see rows of ML experts quietly sitting in front of computer screens in fact about 70% of all orders on Wall street are now placed by software. \n",
    "\n",
    "In this competition, I must predict a signed confidence value, ŷ ti∈[−1,1] , which is multiplied by the market-adjusted return of a given assetCode over a ten day window. If a stock is expected to have a large positive return *compared to the broad market* over the next ten days, a large, positive confidenceValue (near 1.0) should be assigned. If the stock is expected to have a negative return, a large, negative confidenceValue (near -1.0) must be assigned. If unsure, assign it a value near zero.\n",
    "\n",
    "## Question: \n",
    "\n",
    "This kaggle competition aims to predict stock price performance by extracting features from pieces of news. \n",
    "\n",
    "## Datasets\n",
    "\n",
    "This competition is only supported using the Kaggle kernel environment (i.e., we cannot use our PC notebook or other IDE environment). \n",
    "Kaggle provides 2 csv files, one with all the necessary market data and the other with the necessary news information.\n",
    "\n",
    "#### Market\n",
    "\n",
    "This dataset contains market data from February 2007 to December 2016\n",
    "\n",
    "- Data Set Characteristics:\n",
    "    - Number of Instances: 4072956\n",
    "    - Number of Attributes: 16 types (13 numeric, 2 categorical (or text) and 1 datetime)\n",
    "    - Missing Attribute Values: few values from features: returnsClosePrevMktres1, returnsOpenPrevMktres1, returnsClosePrevMktres10, returnsOpenPrevMktres10\n",
    "    - Donor: Market data provided by Intrinio. \n",
    "\n",
    "| **Variable** | **Definition** | **Key** |\n",
    "| ---------| --------- |\n",
    "| time | the current time | datatime64[ns, UTC]|\n",
    "| assetCode | Unique id of an asset | object |\n",
    "| assetName | Name that corresponds to a group of assetCodes | category |\n",
    "| universe | a boolean indicating whether or not the instrument on that day will be included in scoring | float64 |\n",
    "| volume| trading volume in shares for the day| float64 |\n",
    "| close| the close prize for the day | float64 |\n",
    "| open| the open prize for the day | float64 |\n",
    "| returnsClosePrevRaw1| Returns calculated close-to-close for raw data | float64 |\n",
    "| returnsOpenPrevRaw1| Returns calculated open-to-open for raw data | float64 |\n",
    "| returnsClosePrevMktres1| Returns calculated close-to-close for market-residualized (MKtres) for one day | float64 |\n",
    "| returnsOpenPrevMktres1| Returns calculated open-to-open for market-residualized (MKtres) for one day | float64 |\n",
    "| returnsClosePrevRaw10| Returns calculated close-to-close for raw datafor previous 10 days | float64 |\n",
    "| returnsOpenPrevRaw10| Returns calculated open-to-open for raw datafor previous 10 days | float64 |\n",
    "| returnsClosePrevMktres10| Returns calculated close-to-close for market-residualized (MKtres) for 10 days | float64 |\n",
    "| returnsOpenPrevMktres10| Returns calculated open-to-open for market-residualized (MKtres) for previous 10 days | float64 |\n",
    "| returnsOpenNextMktres10| Returns calculated open-to-open for market-residualized (MKtres) for next 10 days | float64 |\n",
    "\n",
    "The **target label** is returnsOpenNextMktres10. In the training set for date t, this is the return from t+1 market open to t+10 market open.\n",
    "\n",
    "#### News\n",
    "\n",
    "Contains news articles/alerts data from January 2007 to December 2016\n",
    "\n",
    "- Data Set Characteristics:\n",
    "    - Number of Instances: 9328750\n",
    "    - Number of Attributes: 35 types (15 numeric, 11 categorical and 3 boolean)\n",
    "    - Missing Attribute Values: Unknown due to memory limit\n",
    "    - Donor: News data provided by Thomson Reuters. Copyright ©, Thomson Reuters, 2017. All Rights Reserved.\n",
    "\n",
    "| **Variable** | **Definition** | **Key** |\n",
    "| ---------  | ---------  |\n",
    "| time  | UTC timestamp of this news item when it was created  | datatime64[ns, UTC] |\n",
    "| firstCreated  | UTC timestamp for the first version of the item | datatime64[ns, UTC]|\n",
    "| sourceId | an Id for each news item| object   |\n",
    "| headline  | the item's headline | object|\n",
    "| urgency  | differentiates story types (1: alert, 3: article) | int8 |\n",
    "| takeSequence  | the take sequence number of the news item, starting at 1. For a given story, alerts and articles have separate sequences. | float64 |\n",
    "| provider | identifier for the organization which provided the news item (e.g. RTRS for Reuters News, BSW for Business Wire) | category |\n",
    "| subjects | topic codes and company identifiers that relate to this news item. Topic codes describe the news item's subject matter. These can cover asset classes, geographies, events, industries/sectors, and other types. | category |\n",
    "| audiences |  identifies which desktop news product(s) the news item belongs to. They are typically tailored to specific audiences. (e.g. \"M\" for Money International News Service and \"FB\" for French General News Service) | category |\n",
    "| bodySize | the size of the current version of the story body in characters | int32 |\n",
    "| companyCount | the number of companies explicitly listed in the news item in the subjects field | int8 |\n",
    "| headlineTag | the Thomson Reuters headline tag for the news item | object |\n",
    "| marketCommentary | boolean indicator that the item is discussing general market conditions, such as \"After the Bell\" summaries | bool |\n",
    "| sentenceCount |  the total number of sentences in the news item. Can be used in conjunction with firstMentionSentence to determine the relative position of the first mention in the item. | int16 |\n",
    "| wordCount | the total number of lexical tokens (words and punctuation) in the news item | int32 |\n",
    "| assetCodes | list of assets mentioned in the item | category |\n",
    "| assetName |  name of the asset | category |\n",
    "| firstMentionSentence | the first sentence, starting with the headline, in which the scored asset is mentioned. 1: headline, 2: first sentence of the story body, 3: second sentence of the body, etc, 0: the asset being scored was not found in the news item's headline or body text. As a result, the entire news item's text (headline + body) will be used to determine the sentiment score.  | int16 |\n",
    "| relevance | a decimal number indicating the relevance of the news item to the asset. It ranges from 0 to 1. If the asset is mentioned in the headline, the relevance is set to 1. When the item is an alert (urgency == 1), relevance should be gauged by firstMentionSentence instead. | float32 |\n",
    "| sentimentClass | indicates the predominant sentiment class for this news item with respect to the asset. The indicated class is the one with the highest probability. | int8 |\n",
    "| sentimentNegative | probability that the sentiment of the news item was negative for the asset | float32 |\n",
    "| sentimentNeutral | probability that the sentiment of the news item was neutral for the asset | float32 |\n",
    "| sentimentPositive | probability that the sentiment of the news item was positive for the asset | float32 |\n",
    "| sentimentWordCount| the number of lexical tokens in the sections of the item text that are deemed relevant to the asset. This can be used in conjunction with wordCount to determine the proportion of the news item discussing the asset. | int32 |\n",
    "| noveltyCount12H | The 12 hour novelty of the content within a news item on a particular asset. It is calculated by comparing it with the asset-specific text over a cache of previous news items that contain the asset. | int16 |\n",
    "| noveltyCount24H| same as above, but for 24 hours | int16 |\n",
    "| noveltyCount3D| same as above, but for 3 day | int16 |\n",
    "| noveltyCount5D| same as above, but for 5 day | int16 |\n",
    "| noveltyCount7D| same as above, but for 7 day | int16 |\n",
    "| volumeCounts12H| the 12 hour volume of news for each asset. A cache of previous news items is maintained and the number of news items that mention the asset within each of five historical periods is calculated. | int16 |\n",
    "| volumeCounts24H| same as above, but for 24 hours | int16 |\n",
    "| volumeCounts3D| same as above, but for 3 days | int16 |\n",
    "| volumeCounts5D| same as above, but for 5 days | int16 |\n",
    "| volumeCounts7D| same as above, but for 7 days | int16 |\n",
    "\n",
    "*ML modeling strategy*\n",
    "\n",
    "I could consider each asset as an ML instance, aggregate each piece of news and roll up the review sentiment into either an average score or a multiclass model.\n",
    "Or I could treat every market date along with every asset (available for that date) as an ML instance and then assign it some measures of spread such as the mean of the news features' scores.\n",
    "\n",
    "## EDA\n",
    "\n",
    "#### Market with normalized variables\n",
    "\n",
    "![**Figure News Corr**](images/market log normalized feats.png)\n",
    "\n",
    "\n",
    "\n",
    "In the above collection of boxes is shown than the open values are closely correlated with the closing values. Also the target value (returnsOpenNextMktres10) is not normalized and so it shows how the other variables were before normalization: With several outliers.\n",
    "\n",
    "![**Figure News Corr**](images/mkt_corr_log.png)\n",
    "\n",
    "\n",
    "\n",
    "In a better view, thanks to the correlogram aside from confirming the open/close relationship it shows how the values in the returns are correct. The returns for day 1 are correlated with those of day 10. \n",
    "\n",
    "#### News with normalized variables\n",
    "![**Figure News Corr**](images/prepro_news_log.png)\n",
    "\n",
    "\n",
    "\n",
    "In the second row of the above graph, the features representing the size of the news piece have pretty much te same range of normalized values however not the same distribution. In the 4th row a graph of each sentiment's polarity where the neutral's median is highest suggesting higher values in each news piece. Finally in the 5th row the novelty count series have pretty much the same values. As for the volume features they show a correct effect of cumulation over time. \n",
    "\n",
    "![**Figure News Corr**](images/news_corr_log.png)\n",
    "\n",
    "\n",
    "In the figure above the features of size prove a high correlation which is not a surprise. In the dimensionality reduction module I might consider removing some. The feature *sentimentClass* is positively correlated with the *sentimentPositive* feature and negatively correlated with the *sentimentNegative* feature. \n",
    "\n",
    "## Data Acquisition\n",
    "\n",
    "Kaggle provides the following functions to retrieve the two dataframes:\n",
    "\n",
    "```python\n",
    "# First let's import the module and create an environment.\n",
    "from kaggle.competitions import twosigmanews\n",
    "# You can only call make_env() once, so don't lose it!\n",
    "env = twosigmanews.make_env()\n",
    "(market_train_df, news_train_df) = env.get_training_data()\n",
    "```\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "### Normalizing taget value\n",
    "\n",
    "It's important to consider that the Market table has the target variable called *returnsOpenNextMktres10* with a domain in the natural numbers. The action is to clip it to be between 0 and 1. 0 if the return is negative and therefore the stock price did not rise and 1 otherwise.  \n",
    "\n",
    "### Normalization:\n",
    "\n",
    "I will use the log transform to normalize these highly skewed features. Since they can have negative values the process will start with a translation by a constant then the transformation by the log.\n",
    "\n",
    "### Trimming dataset from useless columns\n",
    "\n",
    "There are a few assetCodes pertaining to the same asset (assetName), because of this reason and the fact there are assets whose name is unknown and yet they have valid unique asset codes, that I will keep the asset codes and get rid of the variable assetNames. \n",
    "\n",
    "Some Missing Values and their %: \n",
    "    - returnsClosePrevMktres1\t0.392344\n",
    "    - returnsOpenPrevMktres1\t0.392540\n",
    "    - returnsClosePrevMktres10\t2.283599\n",
    "    - returnsOpenPrevMktres10\t2.284680\n",
    "In other words I cannot count with the residualized open-to-open and close-to-close for one day and 10 days since they show a high correaltion with their homologue raw features and unless they show an improvement to the classifier's performance they will be added as an upgrade. \n",
    "\n",
    "\n",
    "### Prep the news and market tables to be merged into one. \n",
    "\n",
    "Since Im using features in both tables it's easier to merge them. The key in common between the tables is the assetCode and time. But first a small manip needs to be performed unto the news table since every row can contain 1 or more asset codes. \n",
    "\n",
    "1. Consolidate times\n",
    "    - The news info needs to be grouped by the day determined by the market column called *'time'*. \n",
    "    - Since merging is based in time: From yesterday at 22h01 til today at 22h\n",
    "        - i.e: \n",
    "            - 2007-01-01 22:00:01+00:00 -> 2007-01-02 \n",
    "            - 2007-01-01 21:59:59+00:00 -> 2007-01-01\n",
    "2. Asset Code expansion\n",
    "    As for the news info, there are pieces of news that refer to 1 or more asset codes, therefore Im going to transform the news table so that it will be indexed by a single and unique asset code. This means that the number of rows will increase adding more redundancy to the training dataset. (Can this be avoided?)\n",
    "    Furthermore I'm leaving out of the feature selection: the headline, take sequence, provider, headlinetag and assetName since at first glance they are not fit to relate with the stock change. \n",
    "    ![**Figure News Corr**](images/news_xpan.png)\n",
    "3. Group the news by their date and median\n",
    "    The pieces of news for a specific asset code and timeframe (market day) will be consolidated into one row as shown below: \n",
    "    ![**Figure News Corr**](images/nws_median.png)\n",
    "    Notice that the feature values have not taken into consideration but rather their metrics of spread. In this case they are going to be median, std, min and max.\n",
    "    Then the news table is merged with the market one by time and assetcode respectively as shown below\n",
    "    ![**Figure News Corr**](images/nws_comp.png)\n",
    "4. Merge the market data with their respective news info by their date and asset code. \n",
    "\n",
    "\n",
    "## Intial research and Results\n",
    "\n",
    "A good place to start is with a Stochastic Gradient Descent (SGD) classifier, using Scikit-Learn’s SGDClassifier class which uses a linear classifier such as SVM or logistic regression with SGD training. \n",
    "\n",
    "``` python\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "```\n",
    "Based on the confusion matrix, the classifier correctly predicted 2609 instances with negative stock returns, and missed 6200. Correctly predicted 6821 instances going upwards but missed 2645. \n",
    "\n",
    "![**Figure News Corr**](images/results1classifier.png)\n",
    "\n",
    "In this project the aim is to have similar values in precision and recall. Increasing recall will make this model to make sure it doesnt miss any stock in the rise, but some of its predictions will be incorrect. The threshold is therefore correctly placed at 0 maximizing the f1 score. \n",
    "\n",
    "The precision/recall curve doesnt have a conventional shape. It shows that the precision will always be between 0.4 and 0.6 no matter the recall. \n",
    "\n",
    "The ROC curve shows the nature of this SDG classifier: just a purely random classifier. \n",
    "\n",
    "These are the results of the evaluation metrics: \n",
    "\n",
    "- Accuracy: 0.51\n",
    "- Precision: 0.52\n",
    "- Recall: 0.72\n",
    "- F1Score: 0.61\n",
    "- AUC: 0.499\n",
    "\n",
    "## Future improvements\n",
    "1. Use from sklearn.preprocessing the LabelEncodes as it transforms categorical vars (asset codes) into numerical. \n",
    "2. The AUC score can be improved by getting the scores of the predictions in probabilities rather than just boolean values. In the same path logit regressors are commonly used to estimate the probability that an instance belongs to a particular class. So the prediction can be made by classifying on whether a stock will rise or sink. \n",
    "3. And most important of all, predicting something as random as the stock returns is not an easy task. There is much work to do to raise the evaluation metrics. Trying other models, refining feature engineering and implementing other strategies such as building ensembles, etc..\n",
    "\n",
    "## Limitations\n",
    "- Since this is a Kernels-only, time-based competition, I'm bound to use the kaggle kernel which is not very practical nor fast. I'm bound to make sure every test I make on their kernel is correctly designed (so there is no time wasting with simple errors). This is designed to simulate the volume, timeline, and the computational burden that real future data will introduce.\n",
    "- The assetCode is not guaranteed to be unique over time. Here I specifically chose AAPL.O because we all know Apple hasn't changed it's ticker symbol. But that's not guaranteed so you have to be very careful. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
