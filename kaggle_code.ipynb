{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "```python\n",
    "# Will reduce data load for code test\n",
    "toy = 1\n",
    "\n",
    "from kaggle.competitions import twosigmanews\n",
    "# You can only call make_env() once, so don't lose it!\n",
    "env = twosigmanews.make_env()\n",
    "print('Done!')\n",
    "\n",
    "(market_train_df, news_train_df) = env.get_training_data()\n",
    "\n",
    "market_train_df.shape, news_train_df.shape\n",
    "\n",
    "# We will reduce the number of samples for memory reasons\n",
    "if toy:\n",
    "    market_train_df = market_train_df.tail(100_000)\n",
    "    news_train_df = news_train_df.tail(300_000)\n",
    "else:\n",
    "    market_train_df = market_train_df.tail(3_000_000)\n",
    "    news_train_df = news_train_df.tail(6_000_000)\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "news_cols_agg = {\n",
    "    'urgency': ['min', 'count'],\n",
    "    'takeSequence': ['max'],\n",
    "    'bodySize': ['min', 'max', 'mean', 'std'],\n",
    "    'wordCount': ['min', 'max', 'mean', 'std'],\n",
    "    'sentenceCount': ['min', 'max', 'mean', 'std'],\n",
    "    'companyCount': ['min', 'max', 'mean', 'std'],\n",
    "    'marketCommentary': ['min', 'max', 'mean', 'std'],\n",
    "    'relevance': ['min', 'max', 'mean', 'std'],\n",
    "    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n",
    "    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n",
    "    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n",
    "    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n",
    "    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n",
    "    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n",
    "    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n",
    "    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n",
    "    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n",
    "    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n",
    "    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n",
    "    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n",
    "    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n",
    "    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "le = None\n",
    "# Split date into before and after 22h (the time used in train data)\n",
    "# E.g: 2007-03-07 23:26:39+00:00 -> 2007-03-08 00:00:00+00:00 (next day)\n",
    "#      2009-02-25 21:00:50+00:00 -> 2009-02-25 00:00:00+00:00 (current day)\n",
    "news_train_df['time'] = (news_train_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n",
    "\n",
    "# Round time of market_train_df to 0h of curret day\n",
    "market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n",
    "\n",
    "    # Join market and news\n",
    "    \n",
    "    # Fix asset codes (str -> list)\n",
    "news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n",
    "\n",
    "# Expand assetCodes\n",
    "assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n",
    "assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n",
    "\n",
    "assert len(assetCodes_index) == len(assetCodes_expanded)\n",
    "df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n",
    "\n",
    "# Create expandaded news (will repeat every assetCodes' row)\n",
    "news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n",
    "news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n",
    "\n",
    "# Free memory\n",
    "del news_train_df, df_assetCodes\n",
    "\n",
    "\n",
    "\n",
    "# Aggregate numerical news features\n",
    "news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n",
    "\n",
    "# Free memory\n",
    "del news_train_df_expanded\n",
    "\n",
    "# Convert to float32 to save memory\n",
    "news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n",
    "\n",
    "# Flat columns\n",
    "news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n",
    "\n",
    "news_train_df_aggregated.head(1)\n",
    "\n",
    "# Join with train\n",
    "market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n",
    "\n",
    "# Free memory\n",
    "del news_train_df_aggregated\n",
    "\n",
    "#return market_train_df\n",
    "x = market_train_df.copy()\n",
    "#del market_train_df\n",
    "\n",
    "# If not label-encoder... encode assetCode\n",
    "if le is None:\n",
    "    # Get those assetCodes that have more than 10 instances in the market_train_df\n",
    "    # And assign them a number (i)\n",
    "    series = x['assetCode']\n",
    "    min_count = 10\n",
    "    vc = series.value_counts()\n",
    "    le_assetCode = {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n",
    "    # Same thing with the assetNames \n",
    "    series = x['assetName']\n",
    "    min_count = 5\n",
    "    vc = series.value_counts()\n",
    "    le_assetName = {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n",
    "else:\n",
    "    # 'unpack' label encoders\n",
    "    le_assetCode, le_assetName = le\n",
    "    \n",
    "\n",
    "# MAP asset codes and names to their attributed number and if NAN then fill with -1\n",
    "x['assetCode'] = x['assetCode'].map(le_assetCode).fillna(-1).astype(int)\n",
    "x['assetName'] = x['assetName'].map(le_assetName).fillna(-1).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    x.drop(columns=['returnsOpenNextMktres10'], inplace=True)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    x.drop(columns=['universe'], inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "x['dayofweek'], x['month'] = x.time.dt.dayofweek, x.time.dt.month\n",
    "x.drop(columns='time', inplace=True)\n",
    "#    x.fillna(-1000,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fix some mixed-type columns\n",
    "for bogus_col in ['marketCommentary_min', 'marketCommentary_max']:\n",
    "    x[bogus_col] = x[bogus_col].astype(float)\n",
    "\n",
    "# return x, (le_assetCode, le_assetName)\n",
    "\n",
    "X = x.copy()\n",
    "del x\n",
    "\n",
    "le = (le_assetCode, le_assetName)\n",
    "\n",
    "y = market_train_df['returnsOpenNextMktres10'].clip(-1, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "\n",
    "# Save universe data for latter use\n",
    "universe = market_train_df['universe']\n",
    "time = market_train_df['time']\n",
    "\n",
    "# Free memory\n",
    "del market_train_df\n",
    "\n",
    "n_train = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train, y_train = X.iloc[:n_train], y.iloc[:n_train]\n",
    "X_valid, y_valid = X.iloc[n_train:], y.iloc[n_train:]\n",
    "\n",
    "# For valid data, keep only those with universe > 0. This will help calculate the metric\n",
    "u_valid = (universe.iloc[n_train:] > 0)\n",
    "t_valid = time.iloc[n_train:]\n",
    "\n",
    "X_valid = X_valid[u_valid]\n",
    "y_valid = y_valid[u_valid]\n",
    "t_valid = t_valid[u_valid]\n",
    "del u_valid\n",
    "\n",
    "# Creat lgb datasets\n",
    "train_cols = X.columns.tolist()\n",
    "categorical_cols = [] # ['assetCode', 'assetName', 'dayofweek', 'month']\n",
    "\n",
    "# Note: y data is expected to be a pandas Series, as we will use its group_by function in `sigma_score`\n",
    "dtrain = lgb.Dataset(X_train.values, y_train, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)\n",
    "dvalid = lgb.Dataset(X_valid.values, y_valid, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)\n",
    "\n",
    "# We will 'inject' an extra parameter in order to have access to df_valid['time'] inside sigma_score without globals\n",
    "dvalid.params = {\n",
    "    'extra_time': t_valid.factorize()[0]\n",
    "}\n",
    "\n",
    "lgb_params = dict(\n",
    "    objective = 'regression_l1',\n",
    "    learning_rate = 0.1,\n",
    "    num_leaves = 127,\n",
    "    max_depth = -1,\n",
    "#     min_data_in_leaf = 1000,\n",
    "#     min_sum_hessian_in_leaf = 10,\n",
    "    bagging_fraction = 0.75,\n",
    "    bagging_freq = 2,\n",
    "    feature_fraction = 0.5,\n",
    "    lambda_l1 = 0.0,\n",
    "    lambda_l2 = 1.0,\n",
    "    metric = 'None', # This will ignore the loss objetive and use sigma_score instead,\n",
    "    seed = 42 # Change for better luck! :)\n",
    ")\n",
    "\n",
    "def sigma_score(preds, valid_data):\n",
    "    df_time = valid_data.params['extra_time']\n",
    "    labels = valid_data.get_label()\n",
    "    \n",
    "#    assert len(labels) == len(df_time)\n",
    "\n",
    "    x_t = preds * labels #  * df_valid['universe'] -> Here we take out the 'universe' term because we already keep only those equals to 1.\n",
    "    \n",
    "    # Here we take advantage of the fact that `labels` (used to calculate `x_t`)\n",
    "    # is a pd.Series and call `group_by`\n",
    "    x_t_sum = x_t.groupby(df_time).sum()\n",
    "    score = x_t_sum.mean() / x_t_sum.std()\n",
    "\n",
    "    return 'sigma_score', score, True\n",
    "\n",
    "evals_result = {}\n",
    "m = lgb.train(lgb_params, dtrain, num_boost_round=1000, valid_sets=(dvalid,), valid_names=('valid',), verbose_eval=25,\n",
    "              early_stopping_rounds=100, feval=sigma_score, evals_result=evals_result)\n",
    "\n",
    "\n",
    "df_result = pd.DataFrame(evals_result['valid'])\n",
    "\n",
    "ax = df_result.plot(figsize=(12, 8))\n",
    "ax.scatter(df_result['sigma_score'].idxmax(), df_result['sigma_score'].max(), marker='+', color='red')\n",
    "\n",
    "num_boost_round, valid_score = df_result['sigma_score'].idxmax()+1, df_result['sigma_score'].max()\n",
    "print(lgb_params)\n",
    "print(f'Best score was {valid_score:.5f} on round {num_boost_round}')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 14))\n",
    "lgb.plot_importance(m, ax=ax[0])\n",
    "lgb.plot_importance(m, ax=ax[1], importance_type='gain')\n",
    "fig.tight_layout()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from dateutil import parser\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def preprocessing(market_df, news_df, s=False):\n",
    "\n",
    "    ## SAMPLE only 2007\n",
    "    if s:\n",
    "        mkt = market_df.loc[(market_df.time>='2007-01-01')&(market_df.time<='2007-07-31')]\n",
    "        nws = news_df.loc[(news_df.time>='2007-01-01')&(news_df.time<='2007-07-31')]\n",
    "    else:\n",
    "        mkt = market_df.copy()\n",
    "        nws = news_df.copy()\n",
    "\n",
    "    ## MKT will have a value of 1 if the return is positive and 0 otherwise. \n",
    "\n",
    "    mkt[\"target\"] = market_df[\"returnsOpenNextMktres10\"] > 0 # .clip(-1, 1)\n",
    "\n",
    "\n",
    "    # Free memory\n",
    "    del market_df, news_df\n",
    "\n",
    "\n",
    "    ## NORMALIZE \n",
    "    norm_mkt_labels = [\"volume\",\"open\",\"close\",'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n",
    "                  'returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\n",
    "    mkt = log_transform(mkt,norm_mkt_labels)\n",
    "    norm_nws_labels = [\"bodySize\",\"sentenceCount\",\"wordCount\",\"firstMentionSentence\",\"sentimentWordCount\",\n",
    "             'noveltyCount12H','noveltyCount24H','noveltyCount3D','noveltyCount5D','noveltyCount7D',\n",
    "             'volumeCounts12H','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D']\n",
    "    nws = log_transform(nws,norm_nws_labels)\n",
    "\n",
    "\n",
    "    ## TRIM COLUMNS\n",
    "\n",
    "    drop_mkt_feats = ['assetName']\n",
    "    #                   'returnsOpenNextMktres10',\n",
    "    #         'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n",
    "    #         'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n",
    "    #         'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n",
    "    #         'returnsClosePrevMktres10','returnsOpenPrevMktres10' ]\n",
    "    mkt.drop(drop_mkt_feats, axis=1, inplace=True)\n",
    "\n",
    "    drop_nws_feats = ['headline','sourceId','sourceTimestamp', 'firstCreated',\n",
    "    'provider', 'subjects', 'audiences','headlineTag', 'assetName', \n",
    "    'takeSequence']\n",
    "    #     'noveltyCount12H', 'noveltyCount24H',\n",
    "    #     'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H',\n",
    "    #     'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D', 'volumeCounts7D']\n",
    "    nws.drop(drop_nws_feats, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    ## CONSOLIDATE TIME\n",
    "        # i.e: 2007-01-01 22:00:01+00:00 -> 2007-01-02 \n",
    "        #      2007-01-01 21:59:59+00:00 -> 2007-01-01\n",
    "\n",
    "    if mkt.time.dtype != 'datetime64[ns, UTC]':\n",
    "        from dateutil import parser\n",
    "        mkt.time = mkt.time.apply(lambda x: parser.parse(x))\n",
    "        nws.time = nws.time.apply(lambda x: parser.parse(x))\n",
    "    else:\n",
    "        nws['time'] = (nws['time'] - np.timedelta64(22,'h')).dt.ceil('1D') #.dt.date \n",
    "        mkt['time'] = mkt['time'].dt.floor('1D')\n",
    "\n",
    "    # --- PRACTICE\n",
    "    ## Reduce the number of features for practicity sake\n",
    "\n",
    "    nws_prediction_col = nws.columns\n",
    "    #['assetCodes','time','sentimentWordCountLog','sentimentNegative','sentimentNeutral', 'sentimentPositive']\n",
    "    mkt_prediction_col = mkt.columns\n",
    "    #['assetCode','time','target','volumeLog','closeLog']\n",
    "    mkt = mkt.loc[:,mkt_prediction_col]\n",
    "    nws = nws.loc[:,nws_prediction_col]\n",
    "\n",
    "    ## Explode the assetcodes in the news dataframe (Need to modify the dictionary to include the necessary feats\n",
    "\n",
    "    nws['assetCodes'] = nws.assetCodes.apply(lambda a: a.replace('{','').replace('}','').replace(\"'\",'').replace(\" \",''))\n",
    "    print('Beginning to explode asset codes...')\n",
    "    nws = pd.concat([pd.DataFrame({'time':row['time'],\n",
    "                                'sentimentWordCount':row['sentimentWordCount'],\n",
    "                                'sentimentNegative':row['sentimentNegative'],\n",
    "                                'sentimentNeutral':row['sentimentNeutral'],\n",
    "                                'sentimentPositive':row['sentimentPositive'], \n",
    "                                'noveltyCount12H':['noveltyCount12H'], \n",
    "                                'noveltyCount24H':row['noveltyCount24H'],\n",
    "                                'noveltyCount3D':row['noveltyCount3D'], \n",
    "                                'noveltyCount5D':row['noveltyCount5D'], \n",
    "                                'noveltyCount7D':row['noveltyCount7D'],\n",
    "                                'volumeCounts12H':row['volumeCounts12H'],\n",
    "                                'volumeCounts24H':row['volumeCounts24H'], \n",
    "                                'volumeCounts3D':row['volumeCounts3D'], \n",
    "                                'volumeCounts5D':row['volumeCounts5D'], \n",
    "                                'volumeCounts7D':row['volumeCounts7D']},\n",
    "                                  index=row['assetCodes'].split(','))              \n",
    "                        for _, row in nws.loc[:,nws_prediction_col].iterrows()]).reset_index()\n",
    "    nws.rename(columns={'index':'assetCode'}, inplace=True)\n",
    "\n",
    "\n",
    "    return nws, mkt\n",
    "    \n",
    "def get_training_dataset(nws,mkt):\n",
    "\n",
    "    # Prepare the news table with the necessary feats\n",
    "    nws = nws.groupby(['time','assetCode']).median()\n",
    "\n",
    "    # Join market and news info by time and asset code\n",
    "    #     Merge with market data\n",
    "    data = pd.merge(mkt, nws,  how='outer', left_on=['time','assetCode'], right_on = ['time','assetCode'])\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Free memory\n",
    "    del nws, mkt\n",
    "\n",
    "    ## FEATURE ENGINEERING \n",
    "\n",
    "    data['dow'] = data.time.dt.dayofweek\n",
    "    data['mnth'] = data.time.dt.month\n",
    "\n",
    "    data.drop(columns='time', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Take care of the assetCode: The assetCode will encode the frequency of its instances. \n",
    "    # A high value means little presence of asset in dataset. \n",
    "    ac = data.assetCode.value_counts()\n",
    "    mac = {v:i for i,v in enumerate(ac.index)}\n",
    "    data.assetCode = data.assetCode.map(mac)\n",
    "\n",
    "    ## MODEL PREP\n",
    "\n",
    "    X = data.copy().reset_index()\n",
    "    X.drop('index', axis=1, inplace=True)\n",
    "    y = (X.target == True)\n",
    "    X.drop('target', inplace=True, axis=1)\n",
    "\n",
    "    print('Dasaset Size:',X.shape)\n",
    "\n",
    "    training_size = np.floor(X.shape[0]*0.8).astype(int)\n",
    "    shuffle_index = np.random.permutation(training_size)\n",
    "\n",
    "    mX_train, mX_test, my_train, my_test = X[:training_size], X[training_size:], y[:training_size], y[training_size:]\n",
    "    mX_train, my_train = mX_train.loc[shuffle_index], my_train.loc[shuffle_index]\n",
    "\n",
    "    X_train = mX_train.as_matrix()\n",
    "    y_train = my_train.as_matrix()\n",
    "\n",
    "    X_test = mX_test.as_matrix()\n",
    "    y_test = my_test.as_matrix()\n",
    "    \n",
    "    del data, X, y\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def make_random_predictions(predictions_df):\n",
    "    predictions_df.confidenceValue = 2.0 * np.random.rand(len(predictions_df)) - 1.0\n",
    "    \n",
    "# Takes a dataframe and the columnns as labels that will log transform them.\n",
    "def log_transform(df,labels):\n",
    "    for label in labels:\n",
    "        a = np.min([0.001, int(np.floor(df[[label]].min().values[0]))])\n",
    "        df[label] = df[[label]].apply(lambda x: np.log(x+1-a))\n",
    "    return df\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# First let's import the module and create an environment.\n",
    "from kaggle.competitions import twosigmanews\n",
    "# You can only call make_env() once, so don't lose it!\n",
    "env = twosigmanews.make_env()\n",
    "(market_train_df, news_train_df) = env.get_training_data()\n",
    "# You can only iterate through a result from `get_prediction_days()` once\n",
    "# so be careful not to lose it once you start iterating.\n",
    "days = env.get_prediction_days()\n",
    "\n",
    "# make_random_predictions(predictions_template_df)\n",
    "# env.predict(predictions_template_df)\n",
    "(market_obs_df, news_obs_df, predictions_template_df) = next(days)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "\n",
    "# Takes 10 min to complete\n",
    "nws, mkt = preprocessing(market_train_df, news_train_df, True)\n",
    "\n",
    "# Takes less than 5 min \n",
    "#nws, mkt = preprocessing(market_obs_df, news_obs_df, False)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "start = time()\n",
    "get_training_dataset(nws,mkt)\n",
    "print('Building training/test set:',time()-start, 'seconds')\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "# Train the SDG's X_train with those samples that are labeled 5 and not 5\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Measuring Accuracy Using Cross-Validation\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "skfolds = StratifiedKFold(n_splits=10, random_state=42)\n",
    "for train_index, test_index in skfolds.split(X_train, y_train):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = (y_train[train_index])\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = (y_train[test_index])\n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Get good examples of assetcodes that have at least 2 news in different dates AND present in the market: \n",
    "\n",
    "#mkt.loc[mkt.assetCode=='CEA.N']\n",
    "ac = nws.loc[(nws.time>='2007-01-03 00:00:00+00:00')&(nws.time<'2007-01-04 00:00:00+00:00')].assetCode.values\n",
    "\n",
    "uc = nws.loc[(nws.time>='2007-01-04 00:00:00+00:00')&(nws.assetCode.isin(ac))].assetCode.unique()\n",
    "\n",
    "# Asset codes that have more than 1 pieces of news in the same date \n",
    "zzz = nws.assetCode.value_counts()\n",
    "dc = zzz[zzz>5].index.values\n",
    "ass_codes = [dif_date for dif_date in uc if dif_date in dc]\n",
    "mkt.loc[mkt.assetCode.isin(ass_codes),'assetCode'].unique()[:10]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
