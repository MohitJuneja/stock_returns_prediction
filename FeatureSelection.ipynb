{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rnd\n",
    "rnd.seed(42)\n",
    "from dateutil import parser\n",
    "from pandas.tseries.offsets import BDay\n",
    "from itertools import chain\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# Metrics AND FUNCTIONS\n",
    "# Standardize the data:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "## CLASSIFIERS LIST\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Results\n",
    "res = pd.DataFrame()\n",
    "\n",
    "# Prepare the news table with the necessary feats\n",
    "# Taking out 'd1':['mean'],'d2':['mean'],\n",
    "f = {'firstMentionSentence':['median','std'],\n",
    "     'sentimentNeutral':['mean','std'], \n",
    "     'noveltyCount12H':['sum'],'noveltyCount24H':['sum'],'noveltyCount3D':['sum'],'noveltyCount5D':['sum'],'noveltyCount7D':['sum'],\n",
    "     'relevance':['median'],  \n",
    "     'companyCount':['median'], \n",
    "     'sentimentNegative':['std'],\n",
    "     'sentimentWordCount':['median']}\n",
    "\n",
    "def pre_processing(mkt, nws):\n",
    "\n",
    "    ## MKT will have a value of 1 if the return is positive and 0 otherwise. \n",
    "    mkt[\"returnsOpenNextMktres10\"] = mkt[\"returnsOpenNextMktres10\"] > 0 # .clip(-1, 1)\n",
    "    mkt.rename(columns={'returnsOpenNextMktres10':'target'}, inplace=True)\n",
    "\n",
    "## CONSOLIDATE TIME TO THE NEXT BUSINESS DAY\n",
    "    \n",
    "        # i.e: 2007-01-01 22:00:01+00:00 -> 2007-01-02 \n",
    "        #      2007-01-01 21:59:59+00:00 -> 2007-01-01\n",
    "    if mkt.time.dtype != 'datetime64[ns, UTC]':\n",
    "        mkt.time = mkt.time.apply(lambda x: parser.parse(x))\n",
    "        nws.time = nws.time.apply(lambda x: parser.parse(x))\n",
    "    nws['time'] = (nws['time'] - np.timedelta64(22,'h')).dt.ceil('1D') #.dt.date \n",
    "    mkt['time'] = mkt['time'].dt.floor('1D')\n",
    "    # Verify if business day, if not, roll to the next B day\n",
    "    offset = BDay()\n",
    "    nws.time = nws.time.apply(lambda x: offset.rollforward(x))\n",
    "\n",
    "    \n",
    "    ## TRIM\n",
    "    drop_mkt_feats = ['assetName','open','returnsClosePrevRaw1','returnsOpenPrevRaw1',\n",
    "                 'returnsClosePrevRaw10','returnsOpenPrevRaw10', \n",
    "                 'returnsClosePrevMktres1']\n",
    "    mkt.drop(drop_mkt_feats, axis=1, inplace=True)\n",
    "    \n",
    "    drop_nws_feats = ['sourceTimestamp', 'firstCreated', 'sourceId', 'headline',\n",
    "    'takeSequence', 'provider', 'subjects', 'audiences','bodySize',\n",
    "    'headlineTag', 'marketCommentary', 'assetName',\n",
    "    'urgency', 'sentenceCount', 'wordCount', 'sentimentClass', 'sentimentPositive', 'volumeCounts12H',\n",
    "    'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D',\n",
    "    'volumeCounts7D']\n",
    "    \n",
    "    nws.drop(drop_nws_feats, axis=1, inplace=True)\n",
    "\n",
    "    return mkt, nws\n",
    "    \n",
    "        \n",
    "def expand_assets(nws):\n",
    "    nws['assetCodes'] = nws['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")   \n",
    "    assetCodes_expanded = list(chain(*nws['assetCodes']))\n",
    "    assetCodes_index = nws.index.repeat( nws['assetCodes'].apply(len) )\n",
    "    df = pd.DataFrame({'idx': assetCodes_index, 'assetCode': assetCodes_expanded})\n",
    "    # Create expandaded news (will repeat every assetCodes' row)\n",
    "    nws_expanded = pd.merge(df, nws, left_on='idx', right_index=True)\n",
    "    nws_expanded.drop(['idx','assetCodes'], axis=1, inplace=True)\n",
    "\n",
    "    return nws_expanded\n",
    "\n",
    "def split_dataset(features, data):\n",
    "    X = StandardScaler().fit_transform(data.loc[:, features].values)\n",
    "    y = np.array(data.target.values).reshape(X.shape[0],1)\n",
    "\n",
    "    training_size = np.floor(X.shape[0]*0.75).astype(int)\n",
    "    \n",
    "    X_train = X[:training_size]\n",
    "    y_train = y[:training_size]\n",
    "    X_test = X[training_size:]\n",
    "    y_test = y[training_size:]\n",
    "    # Many training algorithms are sensitive to the order of the training instances, so it's generally good practice to shuffle them first:\n",
    "    np.random.seed(42)\n",
    "    rnd_idx = np.random.permutation(training_size)\n",
    "    X_train = X_train[rnd_idx]\n",
    "    y_train = y_train[rnd_idx]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def make_random_predictions(predictions_df):\n",
    "    predictions_df.confidenceValue = 2.0 * np.random.rand(len(predictions_df)) - 1.0    \n",
    "    \n",
    "# Results\n",
    "def get_res(res, clf_time, clf_name, X_train, y_test, y_pred, p):\n",
    "    return pd.concat([res,\n",
    "               pd.DataFrame({'data_size':str(X_train.shape),\n",
    "              'ETA': clf_time,\n",
    "              'Acc': accuracy_score(y_test, y_pred),\n",
    "              'Precision': precision_score(y_test, y_pred),\n",
    "              'Recall': recall_score(y_test, y_pred),\n",
    "              'F1': f1_score(y_test, y_pred),\n",
    "              'MSE': mean_squared_error(y_test, y_pred*1),\n",
    "              'AUC': roc_auc_score(y_test, y_pred), \n",
    "              'Params':p}, index=[clf_name])])\n",
    "\n",
    "def prep_res(r):\n",
    "    # Remove the ETA column as well as the Params and data size\n",
    "    r.drop(['ETA','Params','data_size'], inplace=True, axis=1)\n",
    "    r = r.stack().reset_index()\n",
    "    # Join the two indexes together and convert it to a df\n",
    "    # Merge \n",
    "    r['metric'] = r.apply(lambda row: row.level_0+' '+row.level_1, axis=1)\n",
    "    #res.drop(['level_0','level_1'], inplace=True, axis=1)\n",
    "    #res.set_index('metric',inplace=True)\n",
    "    r.columns = ['clf','metric','all','mix']\n",
    "    return r\n",
    "\n",
    "def get_baseline(res, X_train, y_train, X_test, y_test, title):\n",
    "    ## BASELINE  \n",
    "    # Logistic Regression  95s\n",
    "    start = time()\n",
    "    log_clf = LogisticRegression(random_state=42)\n",
    "    log_clf.fit(X_train, y_train)\n",
    "    y_pred = log_clf.predict(X_test)\n",
    "    res = get_res(res, time()-start, 'LogReg', X_train, y_test, y_pred, title)\n",
    "\n",
    "    # SGD\n",
    "    start = time()\n",
    "    sgd_clf = SGDClassifier(random_state=42)\n",
    "    sgd_clf.fit(X_train, y_train)\n",
    "    y_pred = sgd_clf.predict(X_test)\n",
    "    res = get_res(res, time()-start, 'SGD', X_train, y_test, y_pred, title)\n",
    "\n",
    "    # Decision Tree\n",
    "    start = time()\n",
    "    tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "    tree_clf.fit(X_train, y_train)\n",
    "    # tree_clf.predict_proba(X_test)\n",
    "    tree_clf.predict(X_test)\n",
    "    res = get_res(res, time()-start, 'DecTree', X_train, y_test, y_pred, title)\n",
    "\n",
    "#     # Xtra Trees\n",
    "#     start = time()\n",
    "#     xtree_clf = ExtraTreesClassifier(random_state=42)\n",
    "#     xtree_clf.fit(X_train, y_train)\n",
    "#     y_pred = xtree_clf.predict(X_test)\n",
    "#     res = get_res(res, time()-start, 'XTrees', X_train, y_test, y_pred, title)\n",
    "\n",
    "#     # Random Forest\n",
    "#     start = time()\n",
    "#     forest_clf = RandomForestClassifier(random_state=42)\n",
    "#     forest_clf.fit(X_train, y_train)\n",
    "#     y_pred = forest_clf.predict(X_test)\n",
    "#     res = get_res(res, time()-start, 'RandFor', X_train, y_test, y_pred, title)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "6f326d1cd26f67e7473dda7323d3a938ff7494d1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data... This could take a minute.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# First let's import the module and create an environment.\n",
    "from kaggle.competitions import twosigmanews\n",
    "# You can only call make_env() once, so don't lose it!\n",
    "env = twosigmanews.make_env()\n",
    "(market_train_df, news_train_df) = env.get_training_data()\n",
    "# You can only iterate through a result from `get_prediction_days()` once\n",
    "# so be careful not to lose it once you start iterating.\n",
    "# days = env.get_prediction_days()\n",
    "\n",
    "# # make_random_predictions(predictions_template_df)\n",
    "# # env.predict(predictions_template_df)\n",
    "# (market_obs_df, news_obs_df, predictions_template_df) = next(days)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "18675bc7478e5de3f97cc26753ade5cd0c3fc10f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size prior to clearing all the Nans 127113762\n",
      "Preprocessing Completed: 1009.3060801029205 seconds (1170863, 17) \n",
      "Distribution: \n",
      " 0.5065460263070914 0.49345397369290855\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>close</th>\n",
       "      <th>returnsOpenPrevMktres1</th>\n",
       "      <th>returnsClosePrevMktres10</th>\n",
       "      <th>returnsOpenPrevMktres10</th>\n",
       "      <th>target</th>\n",
       "      <th>universe</th>\n",
       "      <th>firstMentionSentence_median</th>\n",
       "      <th>firstMentionSentence_std</th>\n",
       "      <th>sentimentNeutral_mean</th>\n",
       "      <th>sentimentNeutral_std</th>\n",
       "      <th>relevance_median</th>\n",
       "      <th>companyCount_median</th>\n",
       "      <th>sentimentNegative_std</th>\n",
       "      <th>sentimentWordCount_median</th>\n",
       "      <th>novelty_median</th>\n",
       "      <th>novelty_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2606900.0</td>\n",
       "      <td>32.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.313708</td>\n",
       "      <td>0.568264</td>\n",
       "      <td>0.480041</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.093375</td>\n",
       "      <td>418.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2051600.0</td>\n",
       "      <td>11.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0.412758</td>\n",
       "      <td>0.284623</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.220601</td>\n",
       "      <td>26.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1208600.0</td>\n",
       "      <td>18.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.861472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>401800.0</td>\n",
       "      <td>52.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.260806</td>\n",
       "      <td>0.140365</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.102406</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1636100.0</td>\n",
       "      <td>24.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.010410</td>\n",
       "      <td>0.323777</td>\n",
       "      <td>0.124412</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.305538</td>\n",
       "      <td>61.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      volume  close     ...       novelty_median  novelty_std\n",
       "0  2606900.0  32.19     ...                  0.0          0.0\n",
       "1  2051600.0  11.12     ...                 22.0         10.0\n",
       "2  1208600.0  18.02     ...                  0.0          0.0\n",
       "3   401800.0  52.46     ...                  0.0          0.0\n",
       "4  1636100.0  24.52     ...                  9.0          0.0\n",
       "\n",
       "[5 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resample\n",
    "# end date = 2016-12-30 22:00:00+00:00\n",
    "# mkt = market_train_df.loc[(market_train_df.time>='2007-01-01') & (market_train_df.time<='2009-12-31')]\n",
    "# nws = news_train_df.loc[(news_train_df.time>='2007-01-01') & (news_train_df.time<='2009-12-31') ]\n",
    "# mkt = market_train_df.loc[(market_train_df.time>='2010-01-01') & (market_train_df.time<='2012-12-31')]\n",
    "# nws = news_train_df.loc[(news_train_df.time>='2010-01-01') & (news_train_df.time<='2012-12-31') ]\n",
    "# mkt = market_train_df.loc[(market_train_df.time>='2013-01-01') & (market_train_df.time<='2016-12-31')]\n",
    "# nws = news_train_df.loc[(news_train_df.time>='2013-01-01') & (news_train_df.time<='2016-12-31') ]\n",
    "\n",
    "#print(mkt.shape, nws.shape)\n",
    "\n",
    "# 77 s\n",
    "start = time()\n",
    "mkt, nws = pre_processing(market_train_df, news_train_df)\n",
    "# print(time()-start, 'seconds')\n",
    "del market_train_df, news_train_df\n",
    "## Break down the set of assets 10s\n",
    "# start = time()\n",
    "nws = expand_assets(nws)\n",
    "# print('Exploding Assets',time()-start, 'seconds', nws.shape)\n",
    "\n",
    "## FEATURE SELECTION\n",
    "# 18 s\n",
    "# start = time()\n",
    "nws = nws.groupby(['time','assetCode']).agg(f)\n",
    "#print('Aggregating functions',time()-start, 'seconds', nws.shape)\n",
    "# Correct the labels\n",
    "col_name = ['_'.join(title) if isinstance(title, tuple) else title for title in nws.columns ]\n",
    "assert len(col_name) == len(nws.columns)\n",
    "nws.columns = col_name\n",
    "\n",
    "# print(time()-start, 'seconds')\n",
    "\n",
    "# start = time()\n",
    "\n",
    "\n",
    "#print('Feature Selection',time()-start)\n",
    "\n",
    "# 15min 2007 -> 2010, 35 min 2011-> ..., \n",
    "# Regroup the noveltyCount variables into max, min, median and std. \n",
    "#start = time()\n",
    "current_col = [col for col in filter(lambda x: x.startswith('noveltyCount'), nws.columns)]\n",
    "# nws['novelty_max'] = nws[current_col].apply(max, axis=1)\n",
    "nws['novelty_median'] = nws[current_col].apply(np.median, axis=1)\n",
    "nws['novelty_std'] = nws[current_col].apply(np.std, axis=1)\n",
    "nws.drop(current_col, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# current_col = [col for col in filter(lambda x: x.startswith('volumeCounts'), nws.columns)]\n",
    "# nws['volumeCounts_max'] = nws[current_col].apply(max, axis=1)\n",
    "# nws['volumeCounts_min'] = nws[current_col].apply(min, axis=1)\n",
    "# nws.drop(current_col, axis=1, inplace=True)\n",
    "#print('Feature Selection',time()-start)\n",
    "\n",
    "\n",
    "\n",
    "# MERGING\n",
    "# 16 s\n",
    "# start = time()\n",
    "# Join market and news info by time and asset code\n",
    "#     Merge with market data\n",
    "data = pd.merge(mkt, nws,  how='outer', left_on=['time','assetCode'], right_on = ['time','assetCode'])\n",
    "print('Data size prior to clearing all the Nans', data.size)\n",
    "\n",
    "# 2s\n",
    "## FEATURE ENGINEERING \n",
    "# data['dow'] = data.time.dt.dayofweek\n",
    "# data['mnth'] = data.time.dt.month   # Not a good feature\n",
    "\n",
    "\n",
    "## Instance selection: \n",
    "# assert data.loc[(data.target.isnull()) & (data.sentimentClass_median.isnull()),:].shape[0] == 0\n",
    "# Set all Nans to 0\n",
    "\n",
    "\n",
    "data = data.loc[(~data.target.isnull()) & (~data.firstMentionSentence_median.isnull())].fillna(0)\n",
    "data.drop(['time','assetCode'], inplace=True, axis=1)\n",
    "\n",
    "\n",
    "# Reset Index\n",
    "data.reset_index(inplace=True)\n",
    "data.drop('index', axis=1, inplace=True)\n",
    "\n",
    "## RESULTS\n",
    "true_positives, true_negatives = data.target.value_counts()/data.shape[0]\n",
    "print('Preprocessing Completed:',time()-start, 'seconds', data.shape, '\\nDistribution: \\n', true_positives, true_negatives)\n",
    "\n",
    "\n",
    "del nws, mkt\n",
    "    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0581676a856a57a12bdb1c22becb87b5a36a6894"
   },
   "source": [
    "Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "d22daa1b674bb46d6d598a964bfe0709b7b04e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878147, 16) (292716, 16) (878147, 1) (292716, 1)\n"
     ]
    }
   ],
   "source": [
    "# My_PCA\n",
    "# feats1_2007 = ['returnsClosePrevRaw10', 'dow', 'sentimentWordCount_median', 'returnsClosePrevMktres1', 'companyCount_mean', 'sentimentWordCount_std', 'universe', 'sentimentNegative_std', 'volumeCounts_max', 'firstMentionSentence_median', 'volumeCounts_min', 'sentimentNeutral_std', 'novelty_median', 'sentimentPositive_std', 'novelty_max', 'sentimentNegative_mean']\n",
    "# feats2_2011 = ['returnsOpenPrevMktres10', 'sentenceCount_std', 'sentimentWordCount_std', 'returnsOpenPrevMktres1', 'novelty_max', 'volume', 'universe', 'companyCount_median', 'returnsClosePrevRaw1', 'sentimentNeutral_mean', 'sentimentPositive_mean', 'close', 'firstMentionSentence_median', 'relevance_std', 'relevance_median', 'novelty_std', 'sentimentClass_median', 'wordCount_median', 'urgency_mean', 'sentimentPositive_std', 'returnsClosePrevMktres1']\n",
    "# featsRF_2007 = ['wordCount_std', 'companyCount_mean', 'sentenceCount_std','companyCount_median', 'novelty_median', 'bodySize_median','bodySize_std', 'wordCount_median', 'sentenceCount_median','assetCode', 'open', 'relevance_std', 'sentimentNegative_mean','sentimentWordCount_median', 'sentimentNeutral_mean','sentimentPositive_mean', 'volumeCounts_std', 'relevance_median','volume', 'volumeCounts_max', 'volumeCounts_median','firstMentionSentence_median', 'novelty_std', 'urgency_mean','returnsOpenPrevRaw10']\n",
    "# featsRF_2011 = ['returnsOpenPrevMktres10', 'novelty_max', 'relevance_std','novelty_min', 'num_sub_mean', 'num_audi_mean', 'mnth', 'novelty_std','firstMentionSentence_median', 'companyCount_median','returnsClosePrevMktres10', 'open', 'firstMentionSentence_std','returnsClosePrevMktres1']\n",
    "\n",
    "# LDA\n",
    "features = data.columns.difference(['target'])\n",
    "X_train, X_test, y_train, y_test = split_dataset(features, data)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "45458706f2bf567786975de8b08b725b04440531"
   },
   "source": [
    "Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e401df0a1f25ac85d0a8fc2b5fac6b1b0a100b4"
   },
   "outputs": [],
   "source": [
    "# RAndom forest importances\n",
    "start = time()\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "forest_clf.fit(X_train, y_train)\n",
    "importances = forest_clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "# for f in range(X_train.shape[1]):\n",
    "#     print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ad8a2ef65fb5203279f6d33f29b1744d76617d7e"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "# Plot the feature importances of the forest\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices],\n",
    "       color=\"r\", align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c4397d061ace3587b565bbd0667fe4d9b6e1c98"
   },
   "outputs": [],
   "source": [
    "ind = list(indices).index(28)\n",
    "print(np.c_[features[indices[:ind]],importances[:ind]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6af708b12f07c6d10c0b48f3e15930d5b60eca51"
   },
   "outputs": [],
   "source": [
    "# Get Random forest importance feature set performance results \n",
    "forestFeats = features[indices[:ind]]\n",
    "clf = 'ForesFeat_Q2'\n",
    "X_train, X_test, y_train, y_test = split_dataset(forestFeats, data)\n",
    "res = get_baseline(pd.DataFrame(), X_train, y_train, X_test, y_test, clf )\n",
    "res\n",
    "\n",
    "# Get Baseline \n",
    "start = time()\n",
    "res = get_baseline(res, X_train, y_train, X_test, y_test, 'Baseline')\n",
    "print('Baseline',time()-start)\n",
    "\n",
    "# Get Feature importances from all features\n",
    "start = time() # 827 s - 1335 s - \n",
    "res = prep_res(res)\n",
    "for f in features: \n",
    "\n",
    "    nwf = features.difference([f])\n",
    "    X_train, X_test, y_train, y_test = split_dataset(nwf, data)\n",
    "    res_f = get_baseline(pd.DataFrame(), X_train, y_train, X_test, y_test, '_'+f )\n",
    "    res_f = prep_res(res_f)\n",
    "    res_f.drop(['metric','clf'], axis=1, inplace=True)\n",
    "    res_f.rename(columns={'all':f}, inplace=True)\n",
    "    res = pd.merge(res, res_f, on='mix', how='outer')\n",
    "    res[f+'_X'] = (res['all']-res[f]) / res['all']\n",
    "\n",
    "\n",
    "print('FeatureImportances:',time()-start)\n",
    "\n",
    "res.set_index('mix', inplace=True)\n",
    "res.drop(['clf','metric'], axis=1, inplace=True)\n",
    "my_res = res.loc[:,res.columns.str.endswith('_X')].T\n",
    "\n",
    "n_cols = ['LogReg Acc', 'LogReg Precision', 'LogReg Recall', 'LogReg F1',\n",
    "       'LogReg MSE', 'LogReg AUC', 'SGD Acc', 'SGD Precision',\n",
    "       'SGD Recall', 'SGD F1', 'SGD MSE', 'SGD AUC', 'DecTree Acc',\n",
    "       'DecTree Precision', 'DecTree Recall', 'DecTree F1', 'DecTree MSE',\n",
    "       'DecTree AUC', 'XTrees Acc', 'XTrees Precision', 'XTrees Recall',\n",
    "       'XTrees F1', 'XTrees MSE', 'XTrees AUC', 'RandFor Acc',\n",
    "       'RandFor Precision', 'RandFor Recall', 'RandFor F1', 'RandFor MSE',\n",
    "       'RandFor AUC']\n",
    "\n",
    "my_res = res.loc[:,res.columns.str.endswith('_X')].T\n",
    "\n",
    "my_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "046ec5538436c4b5ef49cde9c3d62e61d28b9d8e"
   },
   "source": [
    "*save my_res*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c61d40649e97fb8644f271d5b2f1ec423abd5e4a"
   },
   "outputs": [],
   "source": [
    "best_feats = pd.DataFrame()\n",
    "mt = ['SGD','DecTree', 'XTrees', 'RandFor','LogReg']\n",
    "quarter = 'Q3'\n",
    "for m in mt:\n",
    "\n",
    "    topFeats = my_res.loc[(my_res[m +' Acc']>0)&(my_res[m+' F1']>0)&(my_res[m+' AUC']>0),my_res.columns.str.startswith(m)].sort_values([m+' Acc',m+' AUC',m+' F1'], ascending=False).index.values\n",
    "    \n",
    "    tf = pd.DataFrame(topFeats, columns = [m])\n",
    "    tf.reset_index(inplace=True)\n",
    "    tf.set_index(m, inplace=True)\n",
    "    tf.index.name = 'features'\n",
    "    tf.rename(columns={'index':m+'_'+quarter}, inplace=True)\n",
    "    \n",
    "    best_feats = pd.merge(best_feats, tf, right_index=True, left_index=True, how='outer')\n",
    "    \n",
    "\n",
    "# Get rid of the _X\n",
    "best_feats.reset_index(inplace=True)\n",
    "best_feats['features'] = best_feats.features.apply(lambda x: x[:-2])\n",
    "best_feats.set_index('features', inplace=True)\n",
    "\n",
    "bClfs = ['SGD_Q3','DecTree_Q3','XTrees_Q3','RandFor_Q3','LogReg_Q3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76d38ab248e3abfe998d78b1f3cc8d10fdfa3bbe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's test the features found by XTrees_Q1\n",
    "clf = bClfs[4]\n",
    "new_feats = best_feats.loc[~best_feats[clf].isnull(),[clf]].index.values\n",
    "X_train, X_test, y_train, y_test = split_dataset(new_feats, data)\n",
    "res = get_baseline(pd.DataFrame(), X_train, y_train, X_test, y_test, clf )\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8728a7f7c61af27f3d8b9db5a258badded3be04"
   },
   "source": [
    "*register res*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "98b24aa2daa0beabd2e1bf1808bca5d8d4b6470d"
   },
   "source": [
    "## Get Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "ecfb278daaf56332ddc662cca1b3480996f5ca5f"
   },
   "outputs": [],
   "source": [
    "res = get_baseline(pd.DataFrame(), X_train, y_train, X_test, y_test, 'Baseline')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "6ac143f6bd0dfa3f52b720dc24d726fad854afe7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_size</th>\n",
       "      <th>ETA</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>MSE</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogReg</th>\n",
       "      <td>(878147, 16)</td>\n",
       "      <td>5.783929</td>\n",
       "      <td>0.525790</td>\n",
       "      <td>0.518775</td>\n",
       "      <td>0.704626</td>\n",
       "      <td>0.597584</td>\n",
       "      <td>0.474210</td>\n",
       "      <td>0.525898</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD</th>\n",
       "      <td>(878147, 16)</td>\n",
       "      <td>1.169805</td>\n",
       "      <td>0.514646</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.696785</td>\n",
       "      <td>0.589280</td>\n",
       "      <td>0.485354</td>\n",
       "      <td>0.514756</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecTree</th>\n",
       "      <td>(878147, 16)</td>\n",
       "      <td>3.064497</td>\n",
       "      <td>0.514646</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.696785</td>\n",
       "      <td>0.589280</td>\n",
       "      <td>0.485354</td>\n",
       "      <td>0.514756</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XTrees</th>\n",
       "      <td>(878147, 16)</td>\n",
       "      <td>31.513141</td>\n",
       "      <td>0.508042</td>\n",
       "      <td>0.509634</td>\n",
       "      <td>0.409588</td>\n",
       "      <td>0.454166</td>\n",
       "      <td>0.491958</td>\n",
       "      <td>0.507982</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandFor</th>\n",
       "      <td>(878147, 16)</td>\n",
       "      <td>76.810160</td>\n",
       "      <td>0.510245</td>\n",
       "      <td>0.512380</td>\n",
       "      <td>0.411714</td>\n",
       "      <td>0.456564</td>\n",
       "      <td>0.489755</td>\n",
       "      <td>0.510186</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            data_size        ETA    ...          AUC    Params\n",
       "LogReg   (878147, 16)   5.783929    ...     0.525898  Baseline\n",
       "SGD      (878147, 16)   1.169805    ...     0.514756  Baseline\n",
       "DecTree  (878147, 16)   3.064497    ...     0.514756  Baseline\n",
       "XTrees   (878147, 16)  31.513141    ...     0.507982  Baseline\n",
       "RandFor  (878147, 16)  76.810160    ...     0.510186  Baseline\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c46c47795cc0ccaa318b0e78f1545f807e09147e"
   },
   "source": [
    "## GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c66dc01eba818d72ec2cc9fc35abe64a7a6aa6ef",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "45f24cfd16f7f0bd6fe04b4da01874e811e2ebad",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SGD\n",
    "my_clf = SGDClassifier(random_state=42)\n",
    "param_grid = [{'loss':['hinge','log','modified_huber',], 'penalty' : ['l1','elasticnet'], 'l1_ratio':[0,0.25,0.5,0.75,1],'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1]}]\n",
    "# 2007\n",
    "#alpha=0.01, l1_ratio=0.25, loss='log',penalty='elasticnet'\n",
    "\n",
    "clf_name = 'SGD'\n",
    "start = time()\n",
    "grid_search = GridSearchCV(my_clf, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "clf_time = time()-start\n",
    "y_pred = grid_search.predict(X_test)\n",
    "# Same thing: y_pred2 = grid_search.best_estimator_.predict(X_test)\n",
    "# Results\n",
    "res = get_res(res, clf_time, clf_name, X_train, y_test, y_pred, str(grid_search.best_params_))  #grid_search.best_params_\n",
    "print(res.loc[clf_name].Params)\n",
    "\n",
    "# DecTrees\n",
    "my_clf = DecisionTreeClassifier(random_state=42)\n",
    "param_grid = {'max_leaf_nodes': list(range(91, 99)),'min_samples_split': [1.0, 2], 'min_weight_fraction_leaf':[0,0.01], 'min_samples_leaf':[7,9,11]}\n",
    "# 2008\n",
    "#'max_leaf_nodes': 94, 'min_samples_leaf': 7, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0\n",
    "#2009\n",
    "#{'max_leaf_nodes': 93, 'min_samples_leaf': 7, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.01}\n",
    "#param_grid = {'max_leaf_nodes': [91,93], 'min_samples_leaf': [5,7], 'min_samples_split': [2,3], 'min_weight_fraction_leaf': [0.01, 0.1]}\n",
    "clf_name = 'DT'\n",
    "start = time()\n",
    "grid_search = GridSearchCV(my_clf, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "clf_time = time()-start\n",
    "y_pred = grid_search.predict(X_test)\n",
    "# Same thing: y_pred2 = grid_search.best_estimator_.predict(X_test)\n",
    "# Results\n",
    "res = get_res(res, clf_time, clf_name, X_train, y_test, y_pred, str(grid_search.best_params_))  #grid_search.best_params_\n",
    "\n",
    "my_clf = LogisticRegression(random_state=42)\n",
    "param_grid = [{'C': [0.001,0.01, 0.1, 1, 2], 'penalty':['l1','l2']}]\n",
    "# 2007\n",
    "#C=0.01, penalty='l1'\n",
    "clf_name = 'LR'\n",
    "start = time()\n",
    "grid_search = GridSearchCV(my_clf, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "clf_time = time()-start\n",
    "y_pred = grid_search.predict(X_test)\n",
    "# Same thing: y_pred2 = grid_search.best_estimator_.predict(X_test)\n",
    "# Results\n",
    "res = get_res(res, clf_time, clf_name, X_train, y_test, y_pred, str(grid_search.best_params_))  #grid_search.best_params_\n",
    "\n",
    "my_clf = RandomForestClassifier(random_state=42)\n",
    "param_grid = {'n_estimators':list(range(43,45,47)),'min_samples_split':[11],'max_leaf_nodes':[93,95,97],'min_samples_leaf':[1,2]}\n",
    "# 2007\n",
    "#max_leaf_nodes=91, min_samples_leaf=1,min_samples_split=11,n_estimators=41\n",
    "# 2008\n",
    "#max_leaf_nodes=91, min_samples_leaf=1,min_samples_split=11, n_estimators=49\n",
    "# 2009\n",
    "# max_leaf_nodes= 95, min_samples_leaf= 1,min_samples_split= 11, n_estimators= 49 & 45\n",
    "clf_name = 'RF'\n",
    "start = time()\n",
    "grid_search = GridSearchCV(my_clf, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "clf_time = time()-start\n",
    "y_pred = grid_search.predict(X_test)\n",
    "# Same thing: y_pred2 = grid_search.best_estimator_.predict(X_test)\n",
    "# Results\n",
    "res = get_res(res, clf_time, clf_name, X_train, y_test, y_pred, str(grid_search.best_params_))  #grid_search.best_params_\n",
    "\n",
    "my_clf = ExtraTreesClassifier(random_state=42)\n",
    "param_grid = {'n_estimators':[47,49],'min_samples_split':[9,11],'max_leaf_nodes':[97,99],'min_samples_leaf':[2,3]}\n",
    "\n",
    "# 2008\n",
    "#'max_leaf_nodes': 95, 'min_samples_leaf': 3, 'min_samples_split': 11, 'n_estimators': 49\n",
    "# 2009\n",
    "# max_leaf_nodes=99, min_samples_leaf=3, min_samples_split=9,n_estimators=49\n",
    "clf_name = 'XT'\n",
    "start = time()\n",
    "grid_search = GridSearchCV(my_clf, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "clf_time = time()-start\n",
    "y_pred = grid_search.predict(X_test)\n",
    "# Same thing: y_pred2 = grid_search.best_estimator_.predict(X_test)\n",
    "# Results\n",
    "res = get_res(res, clf_time, clf_name, X_train, y_test, y_pred, str(grid_search.best_params_))  #grid_search.best_params_\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a8abf4a30761d5cbd0094aae93fd7352c89381c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(res.loc[(res.index=='SGD')&(res.Params!='Baseline'),:].Params.values, \n",
    "res.loc['DT'].Params,\n",
    "res.loc['LR'].Params,\n",
    "res.loc['RF'].Params,\n",
    "res.loc['XT'].Params, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b9b6cb155994e876361edb33d78a40663a283bc"
   },
   "outputs": [],
   "source": [
    "# Top classifiers:\n",
    "my_clf1 = RandomForestClassifier(random_state=42,max_leaf_nodes= 97,min_samples_leaf= 2,min_samples_split= 11, n_estimators=43)\n",
    "my_clf2 = LogisticRegression(random_state=42)\n",
    "my_clf3 = DecisionTreeClassifier(max_leaf_nodes= 98,min_samples_leaf= 11,min_samples_split= 2, min_weight_fraction_leaf=43)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4bbbc6393c350d9ca70d3d8a2957b4823177481f"
   },
   "source": [
    "---\n",
    "ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0689453298aeb7acfab8a12f4ba4f6e607286ecf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "# Random Forest\n",
    "start = time()\n",
    "\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('XTree', my_xTree), ('rf1', my_RF1), ('rf2', my_RF2)],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "clf_time = time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e74c78f7e0742fcd20d55ed27ab4812495b9c30"
   },
   "outputs": [],
   "source": [
    "for clf in (my_xTree,my_RF1, my_RF2, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fff093d8d88537b4a477b2c6f3a4876e392beb28"
   },
   "outputs": [],
   "source": [
    "res = get_res(res, clf_time, 'SVote:XT,RF1,RF2', X_train, y_test, y_pred)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "928fbc583d60c92c557943769aecc3f31f2cf57a"
   },
   "source": [
    "Bagging and Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8073351fd3598099bd59129c63cfba626be91845"
   },
   "outputs": [],
   "source": [
    "#\n",
    "start = time()\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "n_estimators=500\n",
    "bag_clf = BaggingClassifier(\n",
    "    RandomForestClassifier(random_state=42, max_leaf_nodes= 95, min_samples_leaf= 1,min_samples_split= 11, n_estimators= 49),\n",
    "    n_estimators=n_estimators, n_jobs=-1, random_state=40) # , oob_score=True, bootstrap=True\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "clf_time = time()-start\n",
    "res = get_res(res, clf_time, 'PRf1', X_train, y_test, y_pred)\n",
    "print(clf_time) #, bag_clf.oob_score_)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4f1fc914c8b1d103f10657eba6ec67187155e98c"
   },
   "source": [
    "Boosting Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c744a2827449ed2fc0e72b8e90360e5bc433b5ea"
   },
   "outputs": [],
   "source": [
    "# Default base estimator\n",
    "#DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "start = time()\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    RandomForestClassifier(random_state=42), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "clf_time = time()-start\n",
    "res = get_res(res, clf_time, 'aBRf-', X_train, y_test, y_pred)\n",
    "print('Scores:',ada_clf.score(X_train, y_train), ada_clf.score(X_test, y_test),'...overfitting?')\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "28705ee384e49208c8b6375811994fedc2dc6c47"
   },
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dfb6cec629f49c50d86c11a886a03e3a433c1b0e"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=49)\n",
    "\n",
    "start = time()\n",
    "#gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, learning_rate=1.0, random_state=42)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "          for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "print(bst_n_estimators)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)\n",
    "gbrt_best.fit(X_train, y_train)\n",
    "y_pred = gbrt_best.predict(X_test)\n",
    "y_pred_b = y_pred>=0.5\n",
    "\n",
    "res = get_res(res, time()-start, 'GBoostBest', X_train, y_test, y_pred_b)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c67f5d857547ca9f4ceed1bd2fdd3e37cf58ee56"
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b075e362b48720c0d1e15c01440928715447497c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
