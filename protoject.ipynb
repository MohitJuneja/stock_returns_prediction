{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Project Predictive Modeling: MVP\n",
    "### Predicting future stock price returns. *(a Kaggle competition from sigma2)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "***Can we actually predict stock prices with Machine Learning?***\n",
    "\n",
    "Investors make educated guesses by analyzing data. They'll read the news, study the company history, industry, trends...  There are lots of data points that go into malking a predction. \n",
    "The prevailing theory is that stock prices are totally random and unpredicatble. A blind-folded monkey throwing darts at a newspaper's financial pages could select a protfolio that could do just as well as one carefully selected by experts. \n",
    "\n",
    "But that raises the question, why do top firms like Morgan Stanley and citigroup hire quantitative analysts to go build predictive models. We have this idea of a trading floor filled with adrenalined-infused men, with loose ties, running around, yelling something into a phone. But these days are more likely to see rows of ML experts quietly sitting in front of computer screens in fact about 70% of all orders on Wall street are now placed by software. \n",
    "\n",
    "In this competition, I must predict a signed confidence value, ŷ ∈[−1,1] , which is multiplied by the market-adjusted return of a given assetCode over a ten day window. If a stock is expected to have a large positive return *compared to the broad market* over the next ten days, a large, positive confidenceValue (near 1.0) should be assigned. If the stock is expected to have a negative return, a large, negative confidenceValue (near -1.0) must be assigned. If unsure, assign it a value near zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: \n",
    "\n",
    "This kaggle competition aims to predict stock price performance by extracting features from pieces of news. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "This competition is only supported using the Kaggle kernel environment (i.e., we cannot use our PC notebook or other IDE environment). \n",
    "Kaggle provides 2 csv files, one with all the necessary market data and the other with the necessary news information.\n",
    "\n",
    "#### Market\n",
    "\n",
    "This dataset contains market data from February 2007 to December 2016\n",
    "\n",
    "- Data Set Characteristics:\n",
    "    - Number of Instances: 4,072,956\n",
    "    - Number of Attributes: 16 types (13 numeric, 2 categorical (or text) and 1 datetime)\n",
    "    - Missing Attribute Values: few values from features: returnsClosePrevMktres1, returnsOpenPrevMktres1, returnsClosePrevMktres10, returnsOpenPrevMktres10\n",
    "    - Donor: Market data provided by Intrinio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Variable** | **Definition** | **Key** |\n",
    "| ---------| --------- |\n",
    "| time | the current time | datatime64[ns, UTC]|\n",
    "| assetCode | Unique id of an asset | object |\n",
    "| assetName | Name that corresponds to a group of assetCodes | category |\n",
    "| universe | a boolean indicating whether or not the instrument on that day will be included in scoring | float64 |\n",
    "| volume| trading volume in shares for the day| float64 |\n",
    "| close| the close prize for the day | float64 |\n",
    "| open| the open prize for the day | float64 |\n",
    "| returnsClosePrevRaw1| Returns calculated close-to-close for raw data | float64 |\n",
    "| returnsOpenPrevRaw1| Returns calculated open-to-open for raw data | float64 |\n",
    "| returnsClosePrevMktres1| Returns calculated close-to-close for market-residualized (MKtres) for one day | float64 |\n",
    "| returnsOpenPrevMktres1| Returns calculated open-to-open for market-residualized (MKtres) for one day | float64 |\n",
    "| returnsClosePrevRaw10| Returns calculated close-to-close for raw datafor previous 10 days | float64 |\n",
    "| returnsOpenPrevRaw10| Returns calculated open-to-open for raw datafor previous 10 days | float64 |\n",
    "| returnsClosePrevMktres10| Returns calculated close-to-close for market-residualized (MKtres) for 10 days | float64 |\n",
    "| returnsOpenPrevMktres10| Returns calculated open-to-open for market-residualized (MKtres) for previous 10 days | float64 |\n",
    "| returnsOpenNextMktres10| Returns calculated open-to-open for market-residualized (MKtres) for next 10 days | float64 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **target label** is returnsOpenNextMktres10. In the training set for date t, this is the return from t+1 market open to t+10 market open.\n",
    "\n",
    "#### News\n",
    "\n",
    "Contains news articles/alerts data from January 2007 to December 2016\n",
    "\n",
    "- Data Set Characteristics:\n",
    "    - Number of Instances: 9,328,750\n",
    "    - Number of Attributes: 35 types (15 numeric, 11 categorical and 3 boolean)\n",
    "    - Missing Attribute Values: Unknown due to memory limit\n",
    "    - Donor: News data provided by Thomson Reuters. Copyright ©, Thomson Reuters, 2017. All Rights Reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Variable** | **Definition** | **Key** |\n",
    "| ---------  | ---------  |\n",
    "| time  | UTC timestamp of this news item when it was created  | datatime64[ns, UTC] |\n",
    "| firstCreated  | UTC timestamp for the first version of the item | datatime64[ns, UTC]|\n",
    "| sourceId | an Id for each news item| object   |\n",
    "| headline  | the item's headline | object|\n",
    "| urgency  | differentiates story types (1: alert, 3: article) | int8 |\n",
    "| takeSequence  | the take sequence number of the news item, starting at 1. For a given story, alerts and articles have separate sequences. | float64 |\n",
    "| provider | identifier for the organization which provided the news item (e.g. RTRS for Reuters News, BSW for Business Wire) | category |\n",
    "| subjects | topic codes and company identifiers that relate to this news item. Topic codes describe the news item's subject matter. These can cover asset classes, geographies, events, industries/sectors, and other types. | category |\n",
    "| audiences |  identifies which desktop news product(s) the news item belongs to. They are typically tailored to specific audiences. (e.g. \"M\" for Money International News Service and \"FB\" for French General News Service) | category |\n",
    "| bodySize | the size of the current version of the story body in characters | int32 |\n",
    "| companyCount | the number of companies explicitly listed in the news item in the subjects field | int8 |\n",
    "| headlineTag | the Thomson Reuters headline tag for the news item | object |\n",
    "| marketCommentary | boolean indicator that the item is discussing general market conditions, such as \"After the Bell\" summaries | bool |\n",
    "| sentenceCount |  the total number of sentences in the news item. Can be used in conjunction with firstMentionSentence to determine the relative position of the first mention in the item. | int16 |\n",
    "| wordCount | the total number of lexical tokens (words and punctuation) in the news item | int32 |\n",
    "| assetCodes | list of assets mentioned in the item | category |\n",
    "| assetName |  name of the asset | category |\n",
    "| firstMentionSentence | the first sentence, starting with the headline, in which the scored asset is mentioned. 1: headline, 2: first sentence of the story body, 3: second sentence of the body, etc, 0: the asset being scored was not found in the news item's headline or body text. As a result, the entire news item's text (headline + body) will be used to determine the sentiment score.  | int16 |\n",
    "| relevance | a decimal number indicating the relevance of the news item to the asset. It ranges from 0 to 1. If the asset is mentioned in the headline, the relevance is set to 1. When the item is an alert (urgency == 1), relevance should be gauged by firstMentionSentence instead. | float32 |\n",
    "| sentimentClass | indicates the predominant sentiment class for this news item with respect to the asset. The indicated class is the one with the highest probability. | int8 |\n",
    "| sentimentNegative | probability that the sentiment of the news item was negative for the asset | float32 |\n",
    "| sentimentNeutral | probability that the sentiment of the news item was neutral for the asset | float32 |\n",
    "| sentimentPositive | probability that the sentiment of the news item was positive for the asset | float32 |\n",
    "| sentimentWordCount| the number of lexical tokens in the sections of the item text that are deemed relevant to the asset. This can be used in conjunction with wordCount to determine the proportion of the news item discussing the asset. | int32 |\n",
    "| noveltyCount12H | The 12 hour novelty of the content within a news item on a particular asset. It is calculated by comparing it with the asset-specific text over a cache of previous news items that contain the asset. | int16 |\n",
    "| noveltyCount24H| same as above, but for 24 hours | int16 |\n",
    "| noveltyCount3D| same as above, but for 3 day | int16 |\n",
    "| noveltyCount5D| same as above, but for 5 day | int16 |\n",
    "| noveltyCount7D| same as above, but for 7 day | int16 |\n",
    "| volumeCounts12H| the 12 hour volume of news for each asset. A cache of previous news items is maintained and the number of news items that mention the asset within each of five historical periods is calculated. | int16 |\n",
    "| volumeCounts24H| same as above, but for 24 hours | int16 |\n",
    "| volumeCounts3D| same as above, but for 3 days | int16 |\n",
    "| volumeCounts5D| same as above, but for 5 days | int16 |\n",
    "| volumeCounts7D| same as above, but for 7 days | int16 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Market with normalized variables\n",
    "\n",
    "![**Figure News Corr**](images/EDA_mkt_normalized_feats.png)\n",
    "\n",
    "\n",
    "\n",
    "In the above collection of boxes is shown than the open values are closely correlated with the closing values. Also the target value (returnsOpenNextMktres10) is not normalized and so it shows how the other variables were before normalization: With several outliers.\n",
    "\n",
    "![**Figure News Corr**](images/mkt_corr_log.png)\n",
    "\n",
    "\n",
    "\n",
    "In a better view, thanks to the correlogram aside from confirming the open/close relationship it shows how the values in the returns are correct. The returns for day 1 are correlated with those of day 10. \n",
    "\n",
    "#### News with normalized variables\n",
    "![**Figure News Corr**](images/prepro_news_log.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second row of the above graph, the features representing the size of the news piece have pretty much te same range of normalized values however not the same distribution. In the 4th row a graph of each sentiment's polarity where the neutral's median is highest suggesting higher values in each news piece. Finally in the 5th row the novelty count series have pretty much the same values. As for the volume features they show a correct effect of cumulation over time. \n",
    "\n",
    "![**Figure News Corr**](images/news_corr_log.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above the features of size prove a high correlation which is not a surprise. In the dimensionality reduction module I might consider removing some. The feature *sentimentClass* is positively correlated with the *sentimentPositive* feature and negatively correlated with the *sentimentNegative* feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "The dataset will be divided in 3 parts, the most important features will be used in a set of common classifiers and the best classifiers will remain in the model. \n",
    "Ensemble and boosting will also take place in the race for the best score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ML modeling strategy*\n",
    "\n",
    "I could consider each asset as an ML instance, aggregate each piece of news and roll up the review sentiment into either an average score or a multiclass model.\n",
    "Or I could treat every market date along with every asset (available for that date) as an ML instance and then assign it some measures of spread such as the mean of the news features' scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema\n",
    "\n",
    "- **Data Acquisition**\n",
    "    - Import the module and create an environment within Kaggle's kernel\n",
    "    - Get the training data into dataframes\n",
    "    - Features briefing. \n",
    "- **Preprocessing**\n",
    "- **EDA**\n",
    "- **Dimensionality Reduction**\n",
    "- **Model Selection**\n",
    "    - Dataset division in training and test. \n",
    "    - Optimization (training) approaches:\n",
    "    - Predicting returnsOpenNextMktres10\n",
    "    - *get_prediction_days* is a generator which loops through each day and provides all market and news observations which occurred since the last data you've received. \n",
    "- **Evaluation**\n",
    "- **Results submission**\n",
    "    - predictions_df: DataFrame which must have the following columns:\n",
    "        - assetCode: The market asset.\n",
    "        - confidenceValue: Your confidence whether the asset will increase or decrease in 10 trading days. All values must be in the range [-1.0, 1.0].\n",
    "    - Store your predictions for the current prediction day with the kaggle function *predict*\n",
    "    - write_submission_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "\n",
    "Kaggle provides the following functions to retrieve the two dataframes:\n",
    "\n",
    "```python\n",
    "# First let's import the module and create an environment.\n",
    "from kaggle.competitions import twosigmanews\n",
    "# You can only call make_env() once, so don't lose it!\n",
    "env = twosigmanews.make_env()\n",
    "(market_train_df, news_train_df) = env.get_training_data()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "#### 1. Memory Management\n",
    "\n",
    "Since memory management is really important in this competition (as their internal kernel doesn't support operations between features or dimensionality reduction algorithms like lle or t-sne), here are a few tricks I am using (with python):\n",
    "\n",
    "- Performing pandas operations inplace as much as possible\n",
    "- Deleting unused frames, series and arrays\n",
    "- *Using manual garbage collection (gc.collect) after deleting objects*\n",
    "- Set universe column to int8 type\n",
    "- Set all float columns from float64 to float32 (maybe float16?)\n",
    "\n",
    "Additionally a memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "\n",
    "#### 2. Normalizing taget value\n",
    "\n",
    "The Market table has the target variable called *returnsOpenNextMktres10* with a domain in the natural numbers. The action is to clip it to be between 0 and 1. The target variable will have a value of 0 if the return is negative and therefore the stock price went down otherwise it will have a value of 1. The objective of this project is to submit the probability that the stock will rise or go down. \n",
    "\n",
    "#### 3. Trimming dataset from useless columns\n",
    "\n",
    "There are a few assetCodes pertaining to the same asset (assetName), because of this reason and the fact there are assets whose name is unknown and yet they have valid unique asset codes, that I will keep the asset codes and get rid of the variable assetNames. \n",
    "\n",
    "Also there are some columns with missing values such as the: *returnsClosePrevMktres1, returnsOpenPrevMktres1, returnsClosePrevMktres10, returnsClosePrevMktres10 and the returnsOpenPrevMktres10*.\n",
    "In other words I cannot count with these residualized open-to-open and close-to-close for one day and 10 days since they show a high correaltion with their homologue raw features and unless they show an improvement to the classifier's performance they will be added as an upgrade. \n",
    "\n",
    "Furthermore some other columns will be removed as I later found out in the feature selection part that they dont have any prediction power. \n",
    "\n",
    "#### 4. Feature Engineering\n",
    "\n",
    "From the remaining columns I could extract more information thanks to the fact that there were more news records within a business day, so I chose the median and standard deviation for some features and the sum of those features that were correlated like the noveltyCount (12H, 24H, ...) to finally calculate their median and std. \n",
    "\n",
    "#### 5. Merging the news with the market data. \n",
    "\n",
    "Since Im using features in both tables it's easier to merge them. The key in common between the tables is the assetCode and time. But first a small manipulation needs to be performed unto the news table since every row can contain 1 or more asset codes. \n",
    "\n",
    "1. Consolidate dates\n",
    "    - The news info needs get rid of the time factor. Since the mar\n",
    "    \n",
    "    by the previous day determined by the market column called *'time'*. \n",
    "    - Since merging is based in time: From yesterday at 22h01 til today at 22h\n",
    "        - i.e: \n",
    "            - 2007-01-01 22:00:01+00:00 -> 2007-01-02 \n",
    "            - 2007-01-01 21:59:59+00:00 -> 2007-01-01\n",
    "            \n",
    "2. Asset Code expansion\n",
    "    As for the news info, there are pieces of news that refer to 1 or more asset codes, therefore Im going to transform the news table so that it will be indexed by a single and unique asset code. This means that the number of rows will increase adding more redundancy to the training dataset. (***Can this be avoided?***)\n",
    "3. Group the news by their date and median\n",
    "    The pieces of news for a specific asset code and timeframe (market day) will be consolidated into one row as shown below: \n",
    "![**Figure News Corr**](images/news_xpan.png)\n",
    "![**Figure News Corr**](images/nws_median.png)\n",
    "    Notice that the feature values have not taken into consideration but rather their metrics of spread. In this case they are going to be median, and std.\n",
    "4. Then the news table is merged with the market one by time and assetcode respectively as shown below\n",
    "![**Figure News Corr**](images/nws_comp.png)\n",
    "\n",
    "In the figure above the date July 7 is not part of the new table even though there are news records for that day. This is because there is no market record for July 7 and thus the news is useless without its label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "The kaggle kernel doesn't have much capacity to perform complex functions like PCA Kernel, GridSearch and ensemble modeling so my first approach is to reduce the number of features. Nonetheless the kernel dies when the whole dataset is preprocessing, so I divided the dataset in 3 partitions, each pertaining to 3 years of data and proceeded to optimize the current feature set. \n",
    "\n",
    "#### 1. Feature (Importance) Selection\n",
    "\n",
    "A score will be assigned to each feature reflecting an increase (or decrease) in the classifier's performance metrics. These metrics are: Accuracy, Area under the Curve (AUC), F1 score (which takes in consideration the Precision and Recall) and Mean Squared Error (MSE). \n",
    "\n",
    "If the score is greater than zero, the feature increases the classifier's prediction power. \n",
    "For every classifier the scores will be calculated and only those scores above 0 will be kept. \n",
    "\n",
    "Then to select the K best features a one score to qualify them all is defined as below: \n",
    "\n",
    "> Score = $^1/_2[(1-MSE)+^1/_3\\sum_{i=1}^{3} p\\_score_i]$ and $p\\_score ∈ [Acc, AUC, F1]$\n",
    "\n",
    "The importance value of remaining features will be considered for every classifier, for example if the clf1 got feature1 as it's first most important feature and feature2 as its second and so on, so for the first partition the list would be like: \n",
    "\n",
    "![**Figure News Corr**](images/feature_selection.png)\n",
    "\n",
    "For each classifier the features whose metric was positive qualifies to be in a feature set that is tested with every classifier. \n",
    "\n",
    "For example the Logistic Regressor for dataset 1 yielded a set of features which were then used to get the performance metrics of the set of classifiers previously established. This yielded a dataframe where each classifier shows its performance metrics using a particular set of features as shown below for the SGD classifier: \n",
    "\n",
    "![**Figure News Corr**](images/Feature_selection_clf.png)\n",
    "\n",
    "The linear SVM with gradient descent (**SGD**) showed with results above the dummy classifier's along with the **Logistic Regressor** and the **Decision Trees** who also happen to have the same feature sets.\n",
    "\n",
    "Overall analyzing the elected feature sets, all of the features in the dataset happened to be included, so the best feature would be that one that is in most feature sets and with a high importance. Below (unscaled) is a scatter plot of the presence of the features in the determined classifiers' feature sets (x) vs its predictive power (the lower the value the more predictive). \n",
    "\n",
    "![**Figure News Corr**](images/unscaled_feature_selection_sets.png)\n",
    "\n",
    "A simple KNN will order the feaures in terms of their performance and presence in the sets. The resulting features are: \n",
    "\n",
    "1. returnsOpenPrevMktres10, \n",
    "2. returnsClosePrevMktres10, \n",
    "3. firstMentionSentence_std, \n",
    "4. sentimentNeutral_mean, \n",
    "5. returnsOpenPrevMktres1, \n",
    "6. firstMentionSentence_median, \n",
    "7. novelty_median, \n",
    "8. volume, \n",
    "9. relevance_median, \n",
    "10. sentimentNeutral_std, \n",
    "11. universe, \n",
    "12. novelty_std, \n",
    "13. companyCount_median, \n",
    "14. close, \n",
    "15. sentimentNegative_std, \n",
    "16. sentimentWordCount_median\n",
    "\n",
    "This feature set has 62% of variables relative to the news which get's in axis with the project's goal in using the news as primary source of predictors. The figure below shows the distributions per class of four of the best features.\n",
    "\n",
    "![**Figure News Corr**](images/top4feats.png)\n",
    "\n",
    "The separability power of these features, as you can see, is not evident. However they make the classifier achieve the highest scores. This drastically reduces the dataset from 49 to 16 columns and the preprocessing script, which couldn't complete (as the kernel kept dying), now finishes within the 16 minute mark. \n",
    "\n",
    "#### 2. PCA & LDA\n",
    "\n",
    "The top 16 features proved to be the most useful in presenting the best results from the whole feature set. Furthermore I performed PCA to see if there could be a better score and the following figure shows the performance of PCA on the number of components versus the overall score (previously mentioned). \n",
    "\n",
    "![**Figure News Corr**](images/OptimalPCA.png)\n",
    "\n",
    "In parallel I also performed LDA on the optimal feature set and the analysis showed that \n",
    "Decision Trees and SGD performed best with LDA and the Logistic regression classifier got the highest score with data that had already gone through PCA with 8 components. So in baseline mode these are the elected classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "#### 1. Hyper-parameter tuning\n",
    "\n",
    "The panorama changed when it came to choose the optimal parameters (through GridSearch) as the highest scores for the 3 classifiers in question came out to be\n",
    "```python\n",
    "clf1 = SGDClassifier(random_state=42, \n",
    "                     alpha=0.01, \n",
    "                     l1_ratio=0.25, \n",
    "                     loss='log', \n",
    "                     penalty='elasticnet')\n",
    "# --> fit(X_train, y_train)\n",
    "\n",
    "clf2 = LogisticRegression(random_state=42)\n",
    "# --> fit(X_triain_pca, y_train)\n",
    "\n",
    "clf3 = DecisionTreeClassifier(random_state=42, \n",
    "                              max_leaf_nodes=91, \n",
    "                              min_samples_leaf=7,\n",
    "                              min_weight_fraction_leaf=0.01)\n",
    "# --> fit(X_train, y_train)\n",
    "\n",
    "clf4 = RandomForestClassifier(random_state=42, \n",
    "                              max_leaf_nodes=93,\n",
    "                              min_samples_leaf=1,\n",
    "                              min_samples_split=11,\n",
    "                              n_estimators=43)\n",
    "# --> fit(X_train_lda, y_train)\n",
    "\n",
    "clf5 = ExtraTreesClassifier(random_state=42, \n",
    "                            max_leaf_nodes=97, \n",
    "                            min_samples_leaf=2,\n",
    "                            min_samples_split=9,\n",
    "                            n_estimators=47)\n",
    "# --> fit(X_train_lda, y_train)\n",
    "```\n",
    "Where the SGD and Decision trees classifiers performed better with the 16 features established in the beginning (the logistic regressor kept its place with the dataset transformed by PCA-8).\n",
    "\n",
    "I also tested the random forests and extra trees classifier but turned out to be outperformed by our trio of classifiers since there is a chance that I can combine these models to get better results through the **voting ensemble method**\n",
    "\n",
    "#### 2. Ensemble Training and Boosting\n",
    "\n",
    "After optimizing the set of 5 classifiers with methods such as bagging, pasting and boosting the best scores were obtained by: \n",
    "![**Figure News Corr**](images/optimized_clfs.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sklearn VotingClassifier I started with the 5 optimized classifiers. Then by getting rid of the lazy classifiers the ensemble score increased outweighing those of all the single classifiers resulting in the SGD and Decision Tress as being the most cohesive and accurate voters. \n",
    "\n",
    "In soft voting the accuracy reached a mere 0.53663. It may be because of not so well calibrated classifiers. Calibrating them also brought worse precision.\n",
    "\n",
    "With hard mode voting, the accuracy gained two hundredths more, positioning on top with **0.5381** and that might be attributed to the fact that using 2 voters exists the posibility of having different predictions. If there is a disagreement between both classifiers, the voting system simply attributed the instance to the class referring a sinking stock. \n",
    "\n",
    "To overcome this problem, the 3rd best classifier's vote became considered. However the VotingClassifier in h mode yielded an error for it doesnt handle predictions of type float. Because of this and the fact that the classifier has to deliver predictions in terms of probability that I had to build a hard mode voting system. Each classifier assigned a higher probability to the class it predicted, then if at least another classifier chose the same class, it became the elected class. To rephrase it in probability terms I took the average of the 3 classifiers' predictions for the elected class.\n",
    "\n",
    "The resulting accuracy decreased in 1 hundredth yielding a score of 0.537183 (a tad higher than the GB itself)\n",
    "Therefore the final metrics: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![**Figure News Corr**](images/Final_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GradientBoosting classifier performs pretty good, it outputs the probabilities of the rising stock. To put it within the specs of the competition, a threshold was set at 0.5 and those instances below it, got converted into the probability the stock will sink. \n",
    "Furthermore as shown in the graph below the Hard Voting method encompassing the GBoost, SGD and the Decision Trees improves the F1 score by keeping the precision and increasing the recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![**Figure News Corr**](images/Final_comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the test phase the score attributed was 0.55455 and the highest score got so far is 0.88167 unfortunately that kernel was not made public."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Future improvements\n",
    "1. There is plenty of room for feature engineering, unfortunately the kernel kept dying whenever an operation involving 2 columns was invoked. \n",
    "2. The score can be improved by using other classification algorithms that have shown promising results throughout Kaggle competitions such as neural networks or XGBoost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## LIMITATIONS\n",
    "- Since this is a Kernels-only, time-based competition, I'm bound to use the kaggle kernel which is not very practical nor fast. I'm bound to make sure every test I make on their kernel is correctly designed (so there is no time wasting with simple errors). This is designed to simulate the volume, timeline, and the computational burden that real future data will introduce.\n",
    "- The assetCode is not guaranteed to be unique over time. Here I specifically chose AAPL.O because we all know Apple hasn't changed it's ticker symbol. But that's not guaranteed so you have to be very careful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
